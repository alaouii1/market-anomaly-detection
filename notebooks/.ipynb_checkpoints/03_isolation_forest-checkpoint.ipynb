{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Isolation Forest Anomaly Detection\n",
    "\n",
    "## What We're Doing\n",
    "\n",
    "We have cryptocurrency data with 6 engineered features. Now we want to find **anomalies** (unusual hours).\n",
    "\n",
    "**Algorithm:** Isolation Forest\n",
    "\n",
    "**Approach:** \n",
    "- PySpark → Load and prepare data (Big Data tool)\n",
    "- sklearn → Run the algorithm (ML tool)\n",
    "\n",
    "**Why this combo?** Spark MLlib doesn't have Isolation Forest built-in. This hybrid approach is common in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Import Libraries & Create Spark Session\n",
    "\n",
    "### What is an import?\n",
    "\n",
    "Python doesn't have everything built-in. We need to load external tools.\n",
    "\n",
    "**Java equivalent:**\n",
    "```java\n",
    "import java.util.ArrayList;\n",
    "```\n",
    "\n",
    "**Python:**\n",
    "```python\n",
    "import pandas\n",
    "```\n",
    "\n",
    "### What is SparkSession?\n",
    "\n",
    "Think of it like a database connection in Java:\n",
    "```java\n",
    "Connection conn = DriverManager.getConnection(...);\n",
    "```\n",
    "\n",
    "SparkSession is your \"connection\" to Spark. Without it, you can't use Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS - Loading the tools we need\n",
    "# ============================================================\n",
    "\n",
    "# PySpark imports\n",
    "# ---------------\n",
    "# SparkSession: The entry point to Spark (like a database connection)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# functions: Built-in functions for data manipulation (like SQL functions)\n",
    "# Example: col(\"name\") refers to a column called \"name\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# VectorAssembler: Combines multiple columns into one vector column\n",
    "# WHY? ML algorithms in Spark expect features as a single vector, not separate columns\n",
    "# Example: [return, volatility, volume] → [0.5, 1.2, 0.8] (one vector)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# StandardScaler: Scales features to have mean=0 and std=1\n",
    "# WHY? Features have different ranges. Without scaling, big numbers dominate.\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# sklearn imports\n",
    "# ---------------\n",
    "# IsolationForest: The anomaly detection algorithm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# numpy: For array operations (sklearn needs numpy arrays)\n",
    "import numpy as np\n",
    "\n",
    "# pandas: We'll convert Spark DataFrame to pandas at the end\n",
    "# WHY? Easier to work with for small results\n",
    "import pandas as pd\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE SPARK SESSION\n",
    "# ============================================================\n",
    "\n",
    "# SparkSession.builder = Start building a session\n",
    "# .appName(\"...\")     = Give it a name (shows up in Spark UI)\n",
    "# .master(\"local[*]\") = Run on local machine, use all CPU cores\n",
    "#                       In a cluster, this would be the cluster address\n",
    "# .getOrCreate()      = Create new session, or reuse if one exists\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IsolationForest_AnomalyDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduce log verbosity (Spark logs A LOT by default)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark Session created!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load the Processed Data\n",
    "\n",
    "### What are we loading?\n",
    "\n",
    "The CSV files you created in Session 2:\n",
    "- `BTCUSDT_1h_processed.csv` (976 rows)\n",
    "- `ETHUSDT_1h_processed.csv` (976 rows)\n",
    "\n",
    "### How Spark reads CSV vs pandas\n",
    "\n",
    "**pandas:**\n",
    "```python\n",
    "df = pd.read_csv(\"file.csv\")\n",
    "```\n",
    "\n",
    "**PySpark:**\n",
    "```python\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "Extra options in Spark:\n",
    "- `header=True` → First row contains column names\n",
    "- `inferSchema=True` → Automatically detect data types (int, float, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD DATA FROM CSV\n",
    "# ============================================================\n",
    "\n",
    "# Path to our processed data files\n",
    "# Note: \"../\" means \"go up one folder\" (from notebooks/ to project root)\n",
    "btc_path = \"../data/processed/BTCUSDT_1h_processed.csv\"\n",
    "eth_path = \"../data/processed/ETHUSDT_1h_processed.csv\"\n",
    "\n",
    "# Read BTC data\n",
    "# spark.read.csv() returns a Spark DataFrame (similar to a database table)\n",
    "btc_df = spark.read.csv(\n",
    "    btc_path,\n",
    "    header=True,       # First row is column names\n",
    "    inferSchema=True   # Auto-detect types (float, int, string)\n",
    ")\n",
    "\n",
    "# Read ETH data\n",
    "eth_df = spark.read.csv(\n",
    "    eth_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"Data loaded!\")\n",
    "print(f\"BTC rows: {btc_df.count()}\")\n",
    "print(f\"ETH rows: {eth_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECT THE DATA\n",
    "# ============================================================\n",
    "\n",
    "# .printSchema() shows column names and data types\n",
    "# Like DESCRIBE in SQL\n",
    "print(\"=== BTC Data Schema ===\")\n",
    "btc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .show(5) displays first 5 rows\n",
    "# Like SELECT * FROM table LIMIT 5\n",
    "print(\"=== First 5 rows of BTC data ===\")\n",
    "btc_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Select Features for Anomaly Detection\n",
    "\n",
    "### Which columns does Isolation Forest need?\n",
    "\n",
    "Our data has 12 columns:\n",
    "- **Original data:** timestamp, open, high, low, close, volume\n",
    "- **Engineered features:** return, log_return, volatility_24h, volume_change, volume_ratio, price_range\n",
    "\n",
    "**We only use the 6 engineered features** for anomaly detection.\n",
    "\n",
    "### Why not use all columns?\n",
    "\n",
    "- `timestamp` = Not a feature, just an identifier\n",
    "- `open, high, low, close` = Raw prices (the engineered features are better)\n",
    "- `volume` = Already captured in volume_change and volume_ratio\n",
    "\n",
    "The 6 engineered features capture the **behavior**, not the raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE FEATURE COLUMNS\n",
    "# ============================================================\n",
    "\n",
    "# These are the 6 features we engineered in Session 2\n",
    "# We'll use ONLY these for anomaly detection\n",
    "\n",
    "feature_columns = [\n",
    "    \"return\",          # Price change %\n",
    "    \"log_return\",      # Log of price change (mathematically nicer)\n",
    "    \"volatility_24h\",  # How \"crazy\" the market was in last 24h\n",
    "    \"volume_change\",   # Volume change %\n",
    "    \"volume_ratio\",    # Current volume vs 24h average\n",
    "    \"price_range\"      # (high - low) / close (intraday volatility)\n",
    "]\n",
    "\n",
    "print(f\"Using {len(feature_columns)} features:\")\n",
    "for i, col_name in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Prepare Data for Machine Learning\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Spark ML algorithms expect features in a **single vector column**, not separate columns.\n",
    "\n",
    "**What we have:**\n",
    "```\n",
    "| return | log_return | volatility_24h | ... |\n",
    "|--------|------------|----------------|-----|\n",
    "| 0.5    | 0.49       | 1.2            | ... |\n",
    "```\n",
    "\n",
    "**What ML needs:**\n",
    "```\n",
    "| features                    |\n",
    "|-----------------------------|\n",
    "| [0.5, 0.49, 1.2, ...]       |\n",
    "```\n",
    "\n",
    "### The Solution: VectorAssembler\n",
    "\n",
    "VectorAssembler takes multiple columns and combines them into one vector column.\n",
    "\n",
    "### Also: Scaling\n",
    "\n",
    "Remember, our features have different ranges:\n",
    "- `return`: typically -5 to +5\n",
    "- `volume_change`: can be -100 to +500\n",
    "\n",
    "**StandardScaler** makes them all comparable by:\n",
    "- Subtracting the mean (centering at 0)\n",
    "- Dividing by standard deviation (same spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4A: COMBINE FEATURES INTO A VECTOR (VectorAssembler)\n",
    "# ============================================================\n",
    "\n",
    "# VectorAssembler configuration:\n",
    "# - inputCols: List of columns to combine\n",
    "# - outputCol: Name of the new vector column\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,    # Our 6 feature columns\n",
    "    outputCol=\"features_raw\"      # Output column name\n",
    ")\n",
    "\n",
    "# Apply to BTC data\n",
    "# .transform() applies the assembler and returns a NEW DataFrame with the extra column\n",
    "btc_assembled = assembler.transform(btc_df)\n",
    "\n",
    "# Let's see what it looks like\n",
    "print(\"After VectorAssembler:\")\n",
    "print(\"(showing only timestamp and the new features_raw column)\")\n",
    "btc_assembled.select(\"timestamp\", \"features_raw\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4B: SCALE THE FEATURES (StandardScaler)\n",
    "# ============================================================\n",
    "\n",
    "# StandardScaler configuration:\n",
    "# - inputCol: The vector column to scale\n",
    "# - outputCol: Name of the scaled vector column\n",
    "# - withMean=True: Subtract mean (center at 0)\n",
    "# - withStd=True: Divide by standard deviation\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withMean=True,   # Center the data at 0\n",
    "    withStd=True     # Scale to unit variance\n",
    ")\n",
    "\n",
    "# Two-step process:\n",
    "# 1. fit() - Calculate mean and std from the data\n",
    "# 2. transform() - Apply the scaling\n",
    "\n",
    "scaler_model = scaler.fit(btc_assembled)           # Learn the scaling parameters\n",
    "btc_scaled = scaler_model.transform(btc_assembled)  # Apply scaling\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(\"(Notice the values are now centered around 0)\")\n",
    "btc_scaled.select(\"timestamp\", \"features_scaled\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Run Isolation Forest\n",
    "\n",
    "### The Handoff: Spark → sklearn\n",
    "\n",
    "Now we need to:\n",
    "1. Convert Spark DataFrame → pandas DataFrame → numpy array\n",
    "2. Run sklearn's IsolationForest\n",
    "3. Get results back\n",
    "\n",
    "### Why this conversion?\n",
    "\n",
    "- sklearn works with **numpy arrays** (simple Python arrays)\n",
    "- Spark DataFrames are **distributed** across multiple machines\n",
    "- We need to **collect** the data to one place for sklearn\n",
    "\n",
    "### Is this okay for Big Data?\n",
    "\n",
    "For 976 rows, absolutely fine. For millions of rows, we'd need a different approach.\n",
    "\n",
    "In your paper, you can mention: \"For larger datasets, we would use distributed implementations or sampling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5A: CONVERT SPARK → NUMPY\n",
    "# ============================================================\n",
    "\n",
    "# Extract the scaled features as a numpy array\n",
    "# \n",
    "# Step by step:\n",
    "# 1. .select(\"features_scaled\") - Get only the features column\n",
    "# 2. .collect() - Bring all data from Spark to local memory\n",
    "# 3. List comprehension - Extract the vector from each row\n",
    "# 4. np.array() - Convert to numpy array\n",
    "\n",
    "# Get the scaled features column\n",
    "features_rows = btc_scaled.select(\"features_scaled\").collect()\n",
    "\n",
    "# Convert to numpy array\n",
    "# row[0] gets the vector from each row\n",
    "# .toArray() converts Spark vector to numpy array\n",
    "X = np.array([row[0].toArray() for row in features_rows])\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"This means: {X.shape[0]} rows (hours) x {X.shape[1]} features\")\n",
    "print(f\"\\nFirst row (first hour's features):\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5B: RUN ISOLATION FOREST\n",
    "# ============================================================\n",
    "\n",
    "# IsolationForest parameters:\n",
    "#\n",
    "# n_estimators=100\n",
    "#   Number of trees in the forest\n",
    "#   More trees = more stable results, but slower\n",
    "#   100 is a good default\n",
    "#\n",
    "# contamination=0.05\n",
    "#   Expected proportion of anomalies in the data\n",
    "#   0.05 = we expect about 5% of data points to be anomalies\n",
    "#   This affects the threshold for labeling something as anomaly\n",
    "#\n",
    "# random_state=42\n",
    "#   Seed for random number generator\n",
    "#   Makes results reproducible (same result every time you run)\n",
    "#   42 is a common choice (Hitchhiker's Guide reference)\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    contamination=0.05,    # Expected 5% anomalies\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model and predict\n",
    "# fit_predict() does two things:\n",
    "# 1. fit() - Learn what \"normal\" looks like\n",
    "# 2. predict() - Label each point as normal (1) or anomaly (-1)\n",
    "\n",
    "predictions = iso_forest.fit_predict(X)\n",
    "\n",
    "print(\"Isolation Forest complete!\")\n",
    "print(f\"\\nPrediction values: {np.unique(predictions)}\")\n",
    "print(\"  1 = Normal\")\n",
    "print(\" -1 = Anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5C: GET ANOMALY SCORES\n",
    "# ============================================================\n",
    "\n",
    "# Besides labels (1 or -1), we can get continuous scores\n",
    "# score_samples() returns the anomaly score for each point\n",
    "#\n",
    "# Score interpretation:\n",
    "# - More negative = More anomalous\n",
    "# - Close to 0 = Normal\n",
    "# - The threshold is around -0.5 (depends on contamination)\n",
    "\n",
    "anomaly_scores = iso_forest.score_samples(X)\n",
    "\n",
    "print(f\"Score range: {anomaly_scores.min():.3f} to {anomaly_scores.max():.3f}\")\n",
    "print(f\"\\nMost anomalous score: {anomaly_scores.min():.3f}\")\n",
    "print(f\"Most normal score: {anomaly_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Analyze Results\n",
    "\n",
    "Let's see:\n",
    "1. How many anomalies were found?\n",
    "2. Which hours are anomalies?\n",
    "3. What makes them anomalous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6A: COUNT ANOMALIES\n",
    "# ============================================================\n",
    "\n",
    "# Count how many of each label\n",
    "n_normal = np.sum(predictions == 1)\n",
    "n_anomaly = np.sum(predictions == -1)\n",
    "total = len(predictions)\n",
    "\n",
    "print(\"=== RESULTS SUMMARY ===\")\n",
    "print(f\"Total data points: {total}\")\n",
    "print(f\"Normal points:     {n_normal} ({100*n_normal/total:.1f}%)\")\n",
    "print(f\"Anomalies:         {n_anomaly} ({100*n_anomaly/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6B: CREATE RESULTS DATAFRAME\n",
    "# ============================================================\n",
    "\n",
    "# Convert Spark DataFrame to pandas for easier analysis\n",
    "# .toPandas() collects all data and converts to pandas DataFrame\n",
    "\n",
    "btc_pandas = btc_df.toPandas()\n",
    "\n",
    "# Add our results as new columns\n",
    "btc_pandas[\"anomaly_score\"] = anomaly_scores\n",
    "btc_pandas[\"anomaly_label\"] = predictions\n",
    "\n",
    "# Create a human-readable label\n",
    "# In Python: condition ? true_value : false_value becomes:\n",
    "#            true_value if condition else false_value\n",
    "btc_pandas[\"is_anomaly\"] = btc_pandas[\"anomaly_label\"].apply(\n",
    "    lambda x: \"ANOMALY\" if x == -1 else \"Normal\"\n",
    ")\n",
    "\n",
    "print(\"Results DataFrame created!\")\n",
    "print(f\"Columns: {list(btc_pandas.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6C: LOOK AT THE ANOMALIES\n",
    "# ============================================================\n",
    "\n",
    "# Filter to only anomalies\n",
    "anomalies = btc_pandas[btc_pandas[\"anomaly_label\"] == -1]\n",
    "\n",
    "# Sort by anomaly score (most anomalous first)\n",
    "anomalies_sorted = anomalies.sort_values(\"anomaly_score\")\n",
    "\n",
    "print(f\"=== TOP 10 MOST ANOMALOUS HOURS ===\")\n",
    "print()\n",
    "\n",
    "# Show the most important columns for the top 10 anomalies\n",
    "columns_to_show = [\"timestamp\", \"close\", \"return\", \"volume_change\", \"volatility_24h\", \"anomaly_score\"]\n",
    "print(anomalies_sorted[columns_to_show].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6D: UNDERSTAND WHY THEY'RE ANOMALIES\n",
    "# ============================================================\n",
    "\n",
    "# Compare anomalies vs normal data\n",
    "# This shows us what makes anomalies different\n",
    "\n",
    "print(\"=== COMPARISON: Anomalies vs Normal ===\")\n",
    "print()\n",
    "\n",
    "normal_data = btc_pandas[btc_pandas[\"anomaly_label\"] == 1]\n",
    "anomaly_data = btc_pandas[btc_pandas[\"anomaly_label\"] == -1]\n",
    "\n",
    "for col in feature_columns:\n",
    "    normal_mean = normal_data[col].mean()\n",
    "    anomaly_mean = anomaly_data[col].mean()\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Normal avg:  {normal_mean:>10.3f}\")\n",
    "    print(f\"  Anomaly avg: {anomaly_mean:>10.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Save Results\n",
    "\n",
    "We'll save the results to a CSV file so we can:\n",
    "1. Use it later for comparison with other algorithms\n",
    "2. Include it in your paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7: SAVE RESULTS TO CSV\n",
    "# ============================================================\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/results\", exist_ok=True)\n",
    "\n",
    "# Save the full results\n",
    "output_path = \"../data/results/BTCUSDT_isolation_forest_results.csv\"\n",
    "btc_pandas.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "print(f\"Total rows: {len(btc_pandas)}\")\n",
    "print(f\"Columns: {len(btc_pandas.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 8: STOP SPARK SESSION\n",
    "# ============================================================\n",
    "\n",
    "# Always stop Spark when done to free up resources\n",
    "# Like closing a database connection\n",
    "\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ISOLATION FOREST COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What We Did\n",
    "\n",
    "1. **Loaded data** using PySpark (Big Data tool)\n",
    "2. **Prepared features** using VectorAssembler and StandardScaler\n",
    "3. **Ran Isolation Forest** using sklearn\n",
    "4. **Found anomalies** - unusual hours in the BTC data\n",
    "5. **Saved results** for later use\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Isolation Forest found approximately 5% of hours as anomalies\n",
    "- These anomalies typically have extreme values in return, volume_change, or volatility\n",
    "- The algorithm considers ALL features together (multivariate)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Run the same process for ETH data\n",
    "- Implement One-Class SVM (next algorithm)\n",
    "- Implement LOF (Local Outlier Factor)\n",
    "- Compare all 4 algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
