{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - K-Means Anomaly Detection\n",
    "\n",
    "## Method 2 of 4: Clustering Approach\n",
    "\n",
    "### What is K-Means?\n",
    "\n",
    "K-Means groups similar data points into **clusters**.\n",
    "\n",
    "**How it works:**\n",
    "1. Choose K (number of clusters)\n",
    "2. Place K random center points\n",
    "3. Assign each data point to nearest center\n",
    "4. Move centers to middle of their points\n",
    "5. Repeat until stable\n",
    "\n",
    "### How to Detect Anomalies\n",
    "\n",
    "**Key insight:** Anomalies are FAR from any cluster center.\n",
    "\n",
    "```\n",
    "    Normal points          Anomaly\n",
    "    (near center)         (far from all)\n",
    "         •  •                  \n",
    "        • ○ •                   ×\n",
    "         • •                  \n",
    "```\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Considers ALL features together | Must choose K |\n",
    "| Fast and scalable | Assumes spherical clusters |\n",
    "| Works in Spark MLlib | Sensitive to outliers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# IMPORTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, lit, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CREATE SPARK SESSION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeans_AnomalyDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✓ Spark Session created\")\n",
    "print(f\"  Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = \"../data/processed/BTCUSDT_1h_processed.csv\"\n",
    "OUTPUT_PATH = \"../data/results/\"\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    \"return\",\n",
    "    \"log_return\",\n",
    "    \"volatility_24h\",\n",
    "    \"volume_change\",\n",
    "    \"volume_ratio\",\n",
    "    \"price_range\"\n",
    "]\n",
    "\n",
    "# K-Means parameters\n",
    "K_CLUSTERS = 3  # Number of clusters\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Clusters: {K_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load and Prepare Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# LOAD DATA\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✓ Data loaded\")\n",
    "print(f\"  Rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# PREPARE FEATURES: VectorAssembler + StandardScaler\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Step 1: Combine features into vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLUMNS,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Step 2: Scale features (mean=0, std=1)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"✓ Features prepared\")\n",
    "print(\"  • VectorAssembler: Combined 6 features into vector\")\n",
    "print(\"  • StandardScaler: Scaled to mean=0, std=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Train K-Means Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# TRAIN K-MEANS MODEL\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "kmeans = KMeans(\n",
    "    k=K_CLUSTERS,\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "kmeans_model = kmeans.fit(df_scaled)\n",
    "\n",
    "print(\"✓ K-Means model trained\")\n",
    "print(f\"  Clusters: {K_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cluster centers\n",
    "print(\"Cluster Centers (scaled feature space):\")\n",
    "print(\"-\" * 60)\n",
    "centers = kmeans_model.clusterCenters()\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Cluster {i}: {[f'{x:.2f}' for x in center]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# ASSIGN POINTS TO CLUSTERS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_clustered = kmeans_model.transform(df_scaled)\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "df_clustered.groupBy(\"cluster\").count().orderBy(\"cluster\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Calculate Distance to Cluster Center\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CALCULATE DISTANCE TO CLUSTER CENTER\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Broadcast cluster centers for distributed computation\n",
    "centers_broadcast = spark.sparkContext.broadcast(centers)\n",
    "\n",
    "# UDF to calculate Euclidean distance\n",
    "def distance_to_center(features, cluster):\n",
    "    \"\"\"Calculate Euclidean distance from point to its cluster center\"\"\"\n",
    "    center = centers_broadcast.value[cluster]\n",
    "    diff = np.array(features) - np.array(center)\n",
    "    return float(np.sqrt(np.sum(diff ** 2)))\n",
    "\n",
    "distance_udf = udf(distance_to_center, DoubleType())\n",
    "\n",
    "# Add distance column\n",
    "df_clustered = df_clustered.withColumn(\n",
    "    \"distance\",\n",
    "    distance_udf(col(\"features\"), col(\"cluster\"))\n",
    ")\n",
    "\n",
    "print(\"✓ Distance calculated\")\n",
    "print(\"\\nDistance distribution:\")\n",
    "df_clustered.select(\"distance\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Flag Anomalies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# DEFINE THRESHOLD: mean + 2*stddev\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "distance_stats = df_clustered.agg(\n",
    "    mean(col(\"distance\")).alias(\"mean_dist\"),\n",
    "    stddev(col(\"distance\")).alias(\"std_dist\")\n",
    ").first()\n",
    "\n",
    "THRESHOLD = distance_stats[\"mean_dist\"] + 2 * distance_stats[\"std_dist\"]\n",
    "\n",
    "print(f\"Distance mean: {distance_stats['mean_dist']:.4f}\")\n",
    "print(f\"Distance std:  {distance_stats['std_dist']:.4f}\")\n",
    "print(f\"Threshold:     {THRESHOLD:.4f}\")\n",
    "print(f\"\\nPoints with distance > {THRESHOLD:.4f} are anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# FLAG ANOMALIES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_clustered = df_clustered.withColumn(\n",
    "    \"is_anomaly\",\n",
    "    when(col(\"distance\") > THRESHOLD, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Count results\n",
    "total = df_clustered.count()\n",
    "anomalies = df_clustered.filter(col(\"is_anomaly\") == 1).count()\n",
    "normal = total - anomalies\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"K-MEANS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total data points:  {total}\")\n",
    "print(f\"Normal:             {normal} ({100*normal/total:.1f}%)\")\n",
    "print(f\"Anomalies:          {anomalies} ({100*anomalies/total:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Examine Anomalies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# VIEW DETECTED ANOMALIES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Sample anomalies detected (sorted by distance):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_clustered.filter(col(\"is_anomaly\") == 1) \\\n",
    "    .select(\"timestamp\", \"close\", \"return\", \"volume_change\", \"cluster\", \"distance\") \\\n",
    "    .orderBy(col(\"distance\").desc()) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Save Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# SAVE RESULTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Prepare result DataFrame\n",
    "df_result = df_clustered.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df_result = df_result.select(\n",
    "    \"row_id\", \"timestamp\", \"close\", \"return\", \"volume_change\",\n",
    "    \"cluster\", \"distance\",\n",
    "    col(\"is_anomaly\").alias(\"anomaly_kmeans\")\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_file = OUTPUT_PATH + \"kmeans_results\"\n",
    "df_result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_file)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# STOP SPARK\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark.stop()\n",
    "print(\"✓ Spark stopped\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"K-MEANS COMPLETE - Proceed to 05_random_forest.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
