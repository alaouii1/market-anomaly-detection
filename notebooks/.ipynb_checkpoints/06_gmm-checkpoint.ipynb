{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Gaussian Mixture Model (GMM) Anomaly Detection\n",
    "\n",
    "## Method 4 of 4: Probabilistic Approach\n",
    "\n",
    "### What is GMM?\n",
    "\n",
    "GMM is like K-Means but uses **probability** instead of hard assignments.\n",
    "\n",
    "| K-Means | GMM |\n",
    "|---------|-----|\n",
    "| Point belongs to ONE cluster | Point has probability for EACH cluster |\n",
    "| Hard boundaries | Soft, overlapping boundaries |\n",
    "| Spherical clusters | Elliptical clusters |\n",
    "\n",
    "### How to Detect Anomalies\n",
    "\n",
    "GMM gives each point a **probability** of fitting the data.\n",
    "\n",
    "- High probability → Fits well → Normal\n",
    "- Low probability → Doesn't fit → **Anomaly**\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Flexible cluster shapes | More complex than K-Means |\n",
    "| Probability scores | Slower to train |\n",
    "| Handles overlapping clusters | Sensitive to initialization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# IMPORTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "import os\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CREATE SPARK SESSION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GMM_AnomalyDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✓ Spark Session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "DATA_PATH = \"../data/processed/BTCUSDT_1h_processed.csv\"\n",
    "OUTPUT_PATH = \"../data/results/\"\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    \"return\", \"log_return\", \"volatility_24h\",\n",
    "    \"volume_change\", \"volume_ratio\", \"price_range\"\n",
    "]\n",
    "\n",
    "# GMM parameters\n",
    "K_COMPONENTS = 3  # Number of Gaussian components\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Components: {K_COMPONENTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load and Prepare Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# LOAD DATA\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✓ Data loaded\")\n",
    "print(f\"  Rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# PREPARE FEATURES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features_raw\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"✓ Features prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Train GMM Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# TRAIN GMM MODEL\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    k=K_COMPONENTS,\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster\",\n",
    "    probabilityCol=\"probability\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gmm_model = gmm.fit(df_scaled)\n",
    "\n",
    "print(\"✓ GMM model trained\")\n",
    "print(f\"  Components: {gmm_model.getK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# MAKE PREDICTIONS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_gmm = gmm_model.transform(df_scaled)\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "df_gmm.groupBy(\"cluster\").count().orderBy(\"cluster\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Calculate Anomaly Score\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# EXTRACT MAXIMUM PROBABILITY\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# The probability column contains probabilities for each cluster\n",
    "# Max probability = how well the point fits its best cluster\n",
    "# Low max probability = anomaly\n",
    "\n",
    "def get_max_prob(prob_vector):\n",
    "    \"\"\"Get maximum probability from probability vector\"\"\"\n",
    "    return float(max(prob_vector))\n",
    "\n",
    "max_prob_udf = udf(get_max_prob, DoubleType())\n",
    "\n",
    "df_gmm = df_gmm.withColumn(\"max_probability\", max_prob_udf(col(\"probability\")))\n",
    "\n",
    "print(\"Max probability distribution:\")\n",
    "df_gmm.select(\"max_probability\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Flag Anomalies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# DEFINE THRESHOLD: Bottom 5% of probabilities\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Use approxQuantile to find the 5th percentile\n",
    "threshold = df_gmm.approxQuantile(\"max_probability\", [0.05], 0.01)[0]\n",
    "\n",
    "print(f\"5th percentile threshold: {threshold:.4f}\")\n",
    "print(f\"\\nPoints with max_probability < {threshold:.4f} are anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# FLAG ANOMALIES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_gmm = df_gmm.withColumn(\n",
    "    \"is_anomaly\",\n",
    "    when(col(\"max_probability\") < threshold, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Count results\n",
    "total = df_gmm.count()\n",
    "anomalies = df_gmm.filter(col(\"is_anomaly\") == 1).count()\n",
    "normal = total - anomalies\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GMM RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total data points:  {total}\")\n",
    "print(f\"Normal:             {normal} ({100*normal/total:.1f}%)\")\n",
    "print(f\"Anomalies:          {anomalies} ({100*anomalies/total:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Examine Anomalies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# VIEW DETECTED ANOMALIES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Sample anomalies detected (sorted by probability):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_gmm.filter(col(\"is_anomaly\") == 1) \\\n",
    "    .select(\"timestamp\", \"close\", \"return\", \"volume_change\", \"cluster\", \"max_probability\") \\\n",
    "    .orderBy(col(\"max_probability\")) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Save Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# SAVE RESULTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "df_result = df_gmm.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df_result = df_result.select(\n",
    "    \"row_id\", \"timestamp\", \"close\", \"return\", \"volume_change\",\n",
    "    \"cluster\", \"max_probability\",\n",
    "    col(\"is_anomaly\").alias(\"anomaly_gmm\")\n",
    ")\n",
    "\n",
    "output_file = OUTPUT_PATH + \"gmm_results\"\n",
    "df_result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_file)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# STOP SPARK\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark.stop()\n",
    "print(\"✓ Spark stopped\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GMM COMPLETE - Proceed to 07_comparison.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
