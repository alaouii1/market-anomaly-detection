{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - K-Means Anomaly Detection\n",
    "\n",
    "## Method 2 of 4: Clustering Approach\n",
    "\n",
    "### What is K-Means?\n",
    "\n",
    "K-Means groups similar data points into **clusters**.\n",
    "\n",
    "**How it works:**\n",
    "1. Choose K (number of clusters)\n",
    "2. Place K random center points\n",
    "3. Assign each data point to nearest center\n",
    "4. Move centers to middle of their points\n",
    "5. Repeat until stable\n",
    "\n",
    "### How to Detect Anomalies\n",
    "\n",
    "**Key insight:** Anomalies are FAR from any cluster center.\n",
    "\n",
    "```\n",
    "    Normal points          Anomaly\n",
    "    (near center)         (far from all)\n",
    "         •  •                  \n",
    "        • ○ •                   ×\n",
    "         • •                  \n",
    "```\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Considers ALL features together | Must choose K |\n",
    "| Fast and scalable | Assumes spherical clusters |\n",
    "| Works in Spark MLlib | Sensitive to outliers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# IMPORTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, lit, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark Session created\n",
      "  Version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CREATE SPARK SESSION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeans_AnomalyDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✓ Spark Session created\")\n",
    "print(f\"  Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration set\n",
      "  Clusters: 3\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = \"../data/processed/BTCUSDT_1h_processed.csv\"\n",
    "OUTPUT_PATH = \"../data/results/\"\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    \"return\",\n",
    "    \"log_return\",\n",
    "    \"volatility_24h\",\n",
    "    \"volume_change\",\n",
    "    \"volume_ratio\",\n",
    "    \"price_range\"\n",
    "]\n",
    "\n",
    "# K-Means parameters\n",
    "K_CLUSTERS = 3  # Number of clusters\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Clusters: {K_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load and Prepare Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded\n",
      "  Rows: 976\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# LOAD DATA\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✓ Data loaded\")\n",
    "print(f\"  Rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Features prepared\n",
      "  • VectorAssembler: Combined 6 features into vector\n",
      "  • StandardScaler: Scaled to mean=0, std=1\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# PREPARE FEATURES: VectorAssembler + StandardScaler\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Step 1: Combine features into vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLUMNS,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Step 2: Scale features (mean=0, std=1)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"✓ Features prepared\")\n",
    "print(\"  • VectorAssembler: Combined 6 features into vector\")\n",
    "print(\"  • StandardScaler: Scaled to mean=0, std=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Train K-Means Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ K-Means model trained\n",
      "  Clusters: 3\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# TRAIN K-MEANS MODEL\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "kmeans = KMeans(\n",
    "    k=K_CLUSTERS,\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "kmeans_model = kmeans.fit(df_scaled)\n",
    "\n",
    "print(\"✓ K-Means model trained\")\n",
    "print(f\"  Clusters: {K_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers (scaled feature space):\n",
      "------------------------------------------------------------\n",
      "Cluster 0: ['-0.04', '-0.04', '-0.06', '-0.19', '-0.28', '-0.32']\n",
      "Cluster 1: ['-2.25', '-2.26', '0.50', '1.25', '1.79', '2.21']\n",
      "Cluster 2: ['1.64', '1.63', '0.25', '0.95', '1.40', '1.49']\n"
     ]
    }
   ],
   "source": [
    "# Show cluster centers\n",
    "print(\"Cluster Centers (scaled feature space):\")\n",
    "print(\"-\" * 60)\n",
    "centers = kmeans_model.clusterCenters()\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Cluster {i}: {[f'{x:.2f}' for x in center]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster distribution:\n",
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0|  827|\n",
      "|      1|   55|\n",
      "|      2|   94|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# ASSIGN POINTS TO CLUSTERS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_clustered = kmeans_model.transform(df_scaled)\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "df_clustered.groupBy(\"cluster\").count().orderBy(\"cluster\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Calculate Distance to Cluster Center\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Distance calculated\n",
      "\n",
      "Distance distribution:\n",
      "+-------+-------------------+\n",
      "|summary|           distance|\n",
      "+-------+-------------------+\n",
      "|  count|                976|\n",
      "|   mean| 1.6190169011920477|\n",
      "| stddev| 1.0225934517820672|\n",
      "|    min|0.18476895384681388|\n",
      "|    max| 13.256936443001278|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CALCULATE DISTANCE TO CLUSTER CENTER\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Broadcast cluster centers for distributed computation\n",
    "centers_broadcast = spark.sparkContext.broadcast(centers)\n",
    "\n",
    "# UDF to calculate Euclidean distance\n",
    "def distance_to_center(features, cluster):\n",
    "    \"\"\"Calculate Euclidean distance from point to its cluster center\"\"\"\n",
    "    center = centers_broadcast.value[cluster]\n",
    "    diff = np.array(features) - np.array(center)\n",
    "    return float(np.sqrt(np.sum(diff ** 2)))\n",
    "\n",
    "distance_udf = udf(distance_to_center, DoubleType())\n",
    "\n",
    "# Add distance column\n",
    "df_clustered = df_clustered.withColumn(\n",
    "    \"distance\",\n",
    "    distance_udf(col(\"features\"), col(\"cluster\"))\n",
    ")\n",
    "\n",
    "print(\"✓ Distance calculated\")\n",
    "print(\"\\nDistance distribution:\")\n",
    "df_clustered.select(\"distance\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Flag Anomalies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance mean: 1.6190\n",
      "Distance std:  1.0226\n",
      "Threshold:     3.6642\n",
      "\n",
      "Points with distance > 3.6642 are anomalies\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# DEFINE THRESHOLD: mean + 2*stddev\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "distance_stats = df_clustered.agg(\n",
    "    mean(col(\"distance\")).alias(\"mean_dist\"),\n",
    "    stddev(col(\"distance\")).alias(\"std_dist\")\n",
    ").first()\n",
    "\n",
    "THRESHOLD = distance_stats[\"mean_dist\"] + 2 * distance_stats[\"std_dist\"]\n",
    "\n",
    "print(f\"Distance mean: {distance_stats['mean_dist']:.4f}\")\n",
    "print(f\"Distance std:  {distance_stats['std_dist']:.4f}\")\n",
    "print(f\"Threshold:     {THRESHOLD:.4f}\")\n",
    "print(f\"\\nPoints with distance > {THRESHOLD:.4f} are anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "K-MEANS RESULTS\n",
      "==================================================\n",
      "Total data points:  976\n",
      "Normal:             946 (96.9%)\n",
      "Anomalies:          30 (3.1%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# FLAG ANOMALIES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_clustered = df_clustered.withColumn(\n",
    "    \"is_anomaly\",\n",
    "    when(col(\"distance\") > THRESHOLD, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Count results\n",
    "total = df_clustered.count()\n",
    "anomalies = df_clustered.filter(col(\"is_anomaly\") == 1).count()\n",
    "normal = total - anomalies\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"K-MEANS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total data points:  {total}\")\n",
    "print(f\"Normal:             {normal} ({100*normal/total:.1f}%)\")\n",
    "print(f\"Anomalies:          {anomalies} ({100*anomalies/total:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Examine Anomalies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample anomalies detected (sorted by distance):\n",
      "------------------------------------------------------------\n",
      "+-------------------+--------+-------------------+------------------+-------+------------------+\n",
      "|timestamp          |close   |return             |volume_change     |cluster|distance          |\n",
      "+-------------------+--------+-------------------+------------------+-------+------------------+\n",
      "|2025-12-26 02:00:00|89199.99|2.021828098690359  |1113.137577476698 |2      |13.256936443001278|\n",
      "|2026-01-21 16:00:00|87602.4 |-2.97452464357405  |93.2422892494962  |1      |9.55552286358597  |\n",
      "|2025-12-18 13:00:00|88851.7 |1.8241005986821657 |611.2129031317518 |2      |8.165375376805965 |\n",
      "|2026-01-19 00:00:00|92711.02|-1.0271033937796825|118.65475569224829|1      |7.469238009654748 |\n",
      "|2026-01-18 23:00:00|93673.14|-1.9115147330416637|503.44753709511343|1      |7.401525051238573 |\n",
      "|2026-01-21 19:00:00|90355.56|2.1134891925145283 |140.496295052039  |2      |7.224497912873434 |\n",
      "|2026-01-13 22:00:00|95757.21|1.7749273732471593 |194.1222209566899 |2      |5.968617403637303 |\n",
      "|2026-01-09 15:00:00|91176.47|1.138624514697728  |309.474239675243  |2      |5.5559726599759385|\n",
      "|2025-12-18 17:00:00|86483.56|-1.7388597268616413|189.18505375930982|1      |5.432847681733476 |\n",
      "|2026-01-14 14:00:00|96493.15|1.5736720878576582 |416.8969813352493 |2      |5.432616872058735 |\n",
      "+-------------------+--------+-------------------+------------------+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# VIEW DETECTED ANOMALIES\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Sample anomalies detected (sorted by distance):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_clustered.filter(col(\"is_anomaly\") == 1) \\\n",
    "    .select(\"timestamp\", \"close\", \"return\", \"volume_change\", \"cluster\", \"distance\") \\\n",
    "    .orderBy(col(\"distance\").desc()) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Save Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to: ../data/results/kmeans_results\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# SAVE RESULTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Prepare result DataFrame\n",
    "df_result = df_clustered.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "df_result = df_result.select(\n",
    "    \"row_id\", \"timestamp\", \"close\", \"return\", \"volume_change\",\n",
    "    \"cluster\", \"distance\",\n",
    "    col(\"is_anomaly\").alias(\"anomaly_kmeans\")\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_file = OUTPUT_PATH + \"kmeans_results\"\n",
    "df_result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_file)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark stopped\n",
      "\n",
      "==================================================\n",
      "K-MEANS COMPLETE - Proceed to 05_random_forest.ipynb\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# STOP SPARK\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark.stop()\n",
    "print(\"✓ Spark stopped\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"K-MEANS COMPLETE - Proceed to 05_random_forest.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
