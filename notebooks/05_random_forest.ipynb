{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Random Forest Anomaly Detection\n",
    "\n",
    "## Method 3 of 4: Classification Approach\n",
    "\n",
    "### What is Random Forest?\n",
    "\n",
    "Random Forest is an **ensemble** of many decision trees.\n",
    "\n",
    "**Decision Tree:** A series of yes/no questions leading to a prediction.\n",
    "\n",
    "**Random Forest:** Build many trees, each sees slightly different data. Final prediction = majority vote.\n",
    "\n",
    "### How to Use for Anomaly Detection\n",
    "\n",
    "**Problem:** Random Forest needs labeled data (\"anomaly\" or \"normal\").\n",
    "\n",
    "**Solution:** Use K-Means results as pseudo-labels!\n",
    "\n",
    "1. Load K-Means results (from previous notebook)\n",
    "2. Train Random Forest on those labels\n",
    "3. Model learns patterns of anomalies\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Handles complex patterns | Needs labeled data |\n",
    "| Shows feature importance | Slower to train |\n",
    "| Resistant to overfitting | Less interpretable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# IMPORTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, monotonically_increasing_id\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import os\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark Session created\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CREATE SPARK SESSION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForest_AnomalyDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✓ Spark Session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration set\n",
      "  Trees: 100\n",
      "  Max depth: 5\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "DATA_PATH = \"../data/processed/BTCUSDT_1h_processed.csv\"\n",
    "KMEANS_RESULTS = \"../data/results/kmeans_results\"\n",
    "OUTPUT_PATH = \"../data/results/\"\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    \"return\", \"log_return\", \"volatility_24h\",\n",
    "    \"volume_change\", \"volume_ratio\", \"price_range\"\n",
    "]\n",
    "\n",
    "# Random Forest parameters\n",
    "NUM_TREES = 100\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Trees: {NUM_TREES}\")\n",
    "print(f\"  Max depth: {MAX_DEPTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load Data and K-Means Labels\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Original data loaded\n",
      "  Rows: 976\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# LOAD ORIGINAL DATA\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "print(\"✓ Original data loaded\")\n",
    "print(f\"  Rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ K-Means results loaded\n",
      "\n",
      "Label distribution:\n",
      "+--------------+-----+\n",
      "|anomaly_kmeans|count|\n",
      "+--------------+-----+\n",
      "|             1|   30|\n",
      "|             0|  946|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# LOAD K-MEANS RESULTS (for labels)\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "kmeans_df = spark.read.csv(KMEANS_RESULTS, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✓ K-Means results loaded\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "kmeans_df.groupBy(\"anomaly_kmeans\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data joined with labels\n",
      "  Rows: 976\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# JOIN DATA WITH LABELS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Get only the label column from K-Means results\n",
    "labels = kmeans_df.select(\"row_id\", col(\"anomaly_kmeans\").alias(\"label\"))\n",
    "\n",
    "# Join with original data\n",
    "df_labeled = df.join(labels, \"row_id\")\n",
    "\n",
    "# Convert label to double (required by Spark ML)\n",
    "df_labeled = df_labeled.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "\n",
    "print(\"✓ Data joined with labels\")\n",
    "print(f\"  Rows: {df_labeled.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Prepare Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Features prepared\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# VECTORASSEMBLER + STANDARDSCALER\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Combine features\n",
    "assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features_raw\")\n",
    "df_assembled = assembler.transform(df_labeled)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"✓ Features prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Train/Test Split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 818 rows\n",
      "Test set:     158 rows\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# SPLIT DATA: 80% TRAIN, 20% TEST\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "train_data, test_data = df_scaled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_data.count()} rows\")\n",
    "print(f\"Test set:     {test_data.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Train Random Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random Forest model trained\n",
      "  Trees: 100\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# TRAIN RANDOM FOREST MODEL\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    numTrees=NUM_TREES,\n",
    "    maxDepth=MAX_DEPTH,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "print(\"✓ Random Forest model trained\")\n",
    "print(f\"  Trees: {rf_model.getNumTrees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "==================================================\n",
      "(Which features matter most for detecting anomalies?)\n",
      "\n",
      "price_range        0.2797 ███████████\n",
      "volume_change      0.2510 ██████████\n",
      "volatility_24h     0.1932 ███████\n",
      "volume_ratio       0.1439 █████\n",
      "return             0.0990 ███\n",
      "log_return         0.0332 █\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# FEATURE IMPORTANCE\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"(Which features matter most for detecting anomalies?)\")\n",
    "print()\n",
    "\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "feature_importance = sorted(zip(FEATURE_COLUMNS, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in feature_importance:\n",
    "    bar = \"█\" * int(importance * 40)\n",
    "    print(f\"{feature:<18} {importance:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluate Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|  154|\n",
      "|       1.0|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# MAKE PREDICTIONS ON TEST SET\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "print(\"Predictions on test set:\")\n",
    "predictions.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:  0.9620 (96.2%)\n",
      "Precision: 0.9635\n",
      "Recall:    0.9620\n",
      "F1 Score:  0.9543\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CALCULATE METRICS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "precision = evaluator_prec.evaluate(predictions)\n",
    "recall = evaluator_rec.evaluate(predictions)\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Apply to Full Dataset & Save\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RANDOM FOREST RESULTS (Full Dataset)\n",
      "==================================================\n",
      "Total data points:  976\n",
      "Normal:             956 (98.0%)\n",
      "Anomalies:          20 (2.0%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# APPLY MODEL TO FULL DATASET\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "df_full_predictions = rf_model.transform(df_scaled)\n",
    "\n",
    "# Count results\n",
    "total = df_full_predictions.count()\n",
    "anomalies = df_full_predictions.filter(col(\"prediction\") == 1).count()\n",
    "normal = total - anomalies\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RANDOM FOREST RESULTS (Full Dataset)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total data points:  {total}\")\n",
    "print(f\"Normal:             {normal} ({100*normal/total:.1f}%)\")\n",
    "print(f\"Anomalies:          {anomalies} ({100*anomalies/total:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to: ../data/results/rf_results\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# SAVE RESULTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "df_result = df_full_predictions.select(\n",
    "    \"row_id\", \"timestamp\", \"close\", \"return\", \"volume_change\",\n",
    "    col(\"prediction\").cast(\"integer\").alias(\"anomaly_rf\")\n",
    ")\n",
    "\n",
    "output_file = OUTPUT_PATH + \"rf_results\"\n",
    "df_result.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_file)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark stopped\n",
      "\n",
      "==================================================\n",
      "RANDOM FOREST COMPLETE - Proceed to 06_gmm.ipynb\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# STOP SPARK\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "spark.stop()\n",
    "print(\"✓ Spark stopped\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RANDOM FOREST COMPLETE - Proceed to 06_gmm.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
